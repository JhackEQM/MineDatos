{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f508be41-359f-4f12-a4b4-ec60cc0a0312",
   "metadata": {},
   "source": [
    "# üß† Predicci√≥n de Costos M√©dicos ‚Äî *Paso 1: Carga y Preparaci√≥n (Insurance)*\n",
    "\n",
    "## üìò Introducci√≥n (Code1)\n",
    "\n",
    "Este bloque carga el dataset **RegresionTarea.csv** (Insurance de Kaggle), audita su estructura, convierte variables categ√≥ricas a *dummies* y separa **X** e **y** con `TARGET = \"charges\"`.\n",
    "\n",
    "### üîß Qu√© hace el c√≥digo\n",
    "- **Lectura y verificaci√≥n:** `pd.read_csv(DATA_FILE)` y `assert` para asegurar que el archivo existe.  \n",
    "- **Auditor√≠a inicial:** `df.info()` para ver filas, tipos y faltantes.  \n",
    "- **Codificaci√≥n de categ√≥ricas:** `pd.get_dummies(df, drop_first=True)` sobre `sex`, `smoker`, `region` para evitar colinealidad.  \n",
    "- **Separaci√≥n:** `y = df[\"charges\"]` y `X = df.drop(columns=[\"charges\"])`.  \n",
    "- **Chequeo de forma/estad√≠sticos:** imprime `Shape` de `X` y media/desviaci√≥n/rango de `y`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Resultados del Code1 (con tu salida)\n",
    "\n",
    "### üìä Estructura original (`df.info()`)\n",
    "- **Filas:** 1338  \n",
    "- **Columnas:** 7  \n",
    "- **Tipos:**  \n",
    "  - Num√©ricas: `age (int64)`, `bmi (float64)`, `children (int64)`, `charges (float64)`  \n",
    "  - Categ√≥ricas: `sex (object)`, `smoker (object)`, `region (object)`  \n",
    "- **Faltantes:** **ninguno** en ninguna columna  \n",
    "- **Memoria:** ~73.3 KB\n",
    "\n",
    "### üî£ Tras *get_dummies* (`drop_first=True`)\n",
    "- `sex` (2 niveles) ‚Üí **1 dummy**  \n",
    "- `smoker` (2 niveles) ‚Üí **1 dummy**  \n",
    "- `region` (4 niveles) ‚Üí **3 dummies**  \n",
    "- + Num√©ricas originales (`age`, `bmi`, `children`)  \n",
    "- **Total features en X:** **8**  \n",
    "- **Forma de X:** `(1338, 8)` ‚úÖ\n",
    "\n",
    "### üíµ Estad√≠sticos de la variable objetivo `y = charges`\n",
    "- **Media:** **13,270.4223**  \n",
    "- **Desviaci√≥n est√°ndar:** **12,110.0112**  \n",
    "- **Rango:** **[1,121.8739, 63,770.4280]**\n",
    "\n",
    "**Lectura r√°pida:** la dispersi√≥n es alta y el rango muy amplio ‚Üí probable **asimetr√≠a positiva** (cola derecha), t√≠pica en costos m√©dicos (p. ej., fumadores elevan la cola).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Implicancias y siguientes pasos\n",
    "- Considerar **transformar** el objetivo para modelos lineales: `y_log = log1p(charges)` para estabilizar varianza.  \n",
    "- Evaluar **MAE** junto con **RMSE** (RMSE es sensible a outliers).  \n",
    "- EDA recomendado: histogramas/boxplots de `charges`, comparaci√≥n por `smoker` y `region`, y correlaciones con `age`/`bmi`.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3634a416-4dee-4529-87b4-c3befe44a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1338 non-null   int64  \n",
      " 1   sex       1338 non-null   object \n",
      " 2   bmi       1338 non-null   float64\n",
      " 3   children  1338 non-null   int64  \n",
      " 4   smoker    1338 non-null   object \n",
      " 5   region    1338 non-null   object \n",
      " 6   charges   1338 non-null   float64\n",
      "dtypes: float64(2), int64(2), object(3)\n",
      "memory usage: 73.3+ KB\n",
      "Shape: (1338, 8) | y(mean): 13270.4223 | y(std): 12110.0112 | y[min,max]: (1121.8739, 63770.428)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1) Cargar datos y objetivo (Insurance dataset)\n",
    "# =========================================\n",
    "import os, json, warnings, platform, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Nombre del archivo descargado de Kaggle\n",
    "DATA_FILE = \"RegresionTarea.csv\"     # <-- tu archivo limpio de Kaggle\n",
    "TARGET    = \"charges\"           # variable objetivo\n",
    "\n",
    "assert os.path.exists(DATA_FILE), f\"No se encuentra {DATA_FILE}\"\n",
    "\n",
    "# Leer dataset\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "df.info()\n",
    "\n",
    "# Variables categ√≥ricas: sex, smoker, region ‚Üí convertir a dummies (0/1)\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Definir X y y\n",
    "y  = df[TARGET]\n",
    "X  = df.drop(columns=[TARGET])\n",
    "\n",
    "print(\"Shape:\", X.shape,\n",
    "      \"| y(mean):\", round(y.mean(), 4),\n",
    "      \"| y(std):\", round(y.std(), 4),\n",
    "      \"| y[min,max]:\", (round(y.min(), 4), round(y.max(), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156d222a-9435-457c-aba8-915815a5f8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      age     bmi  children  sex_male  smoker_yes  region_northwest  \\\n",
       " 0      19  27.900         0     False        True             False   \n",
       " 1      18  33.770         1      True       False             False   \n",
       " 2      28  33.000         3      True       False             False   \n",
       " 3      33  22.705         0      True       False              True   \n",
       " 4      32  28.880         0      True       False              True   \n",
       " ...   ...     ...       ...       ...         ...               ...   \n",
       " 1333   50  30.970         3      True       False              True   \n",
       " 1334   18  31.920         0     False       False             False   \n",
       " 1335   18  36.850         0     False       False             False   \n",
       " 1336   21  25.800         0     False       False             False   \n",
       " 1337   61  29.070         0     False        True              True   \n",
       " \n",
       "       region_southeast  region_southwest  \n",
       " 0                False              True  \n",
       " 1                 True             False  \n",
       " 2                 True             False  \n",
       " 3                False             False  \n",
       " 4                False             False  \n",
       " ...                ...               ...  \n",
       " 1333             False             False  \n",
       " 1334             False             False  \n",
       " 1335              True             False  \n",
       " 1336             False              True  \n",
       " 1337             False             False  \n",
       " \n",
       " [1338 rows x 8 columns],\n",
       " 0       16884.92400\n",
       " 1        1725.55230\n",
       " 2        4449.46200\n",
       " 3       21984.47061\n",
       " 4        3866.85520\n",
       "            ...     \n",
       " 1333    10600.54830\n",
       " 1334     2205.98080\n",
       " 1335     1629.83350\n",
       " 1336     2007.94500\n",
       " 1337    29141.36030\n",
       " Name: charges, Length: 1338, dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc3b11-2db1-4c3f-81ad-ede13d6ff0af",
   "metadata": {},
   "source": [
    "# üîÄ Split temprano (80/20) ‚Äî *Code2*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Funci√≥n:** `train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE)`\n",
    "  - Separa el dataset en **entrenamiento (80%)** y **prueba (20%)**.\n",
    "  - Usa la misma semilla (`RANDOM_STATE=42`) para **reproducibilidad**.\n",
    "- **Objetivo del split temprano:** fijar un **conjunto de prueba independiente** desde el inicio para evaluar el rendimiento **fuera de muestra** sin sesgos.\n",
    "- **Nota (regresi√≥n):** no se usa `stratify` (aplica a clasificaci√≥n). En regresi√≥n, es buena pr√°ctica verificar que la **distribuci√≥n de `y`** en *train/test* sea similar (ver chequeos abajo).\n",
    "- **Prevenci√≥n de *leakage*:** hacer el split **antes** de cualquier ajuste dependiente de los datos (escalado, selecci√≥n de variables, imputaci√≥n, tuning, etc.), idealmente encapsulados luego en un **pipeline**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado obtenido\n",
    "- **Train:** `(1070, 8)` ‚Üí 1,070 filas y 8 *features*  \n",
    "- **Test:** `(268, 8)` ‚Üí 268 filas y 8 *features*  \n",
    "‚úîÔ∏è Las proporciones **80/20** coinciden con lo esperado y los tama√±os suman las 1,338 observaciones totales.\n",
    "\n",
    "### üß† Interpretaci√≥n\n",
    "- Con ~**1,070** ejemplos para entrenar, hay suficiente datos para un **baseline** con CV y para **tuning** moderado.\n",
    "- **268** ejemplos en *test* proveen una evaluaci√≥n **estable** (varianza de m√©tricas razonable).\n",
    "- Pr√≥ximo paso recomendado: confirmar que *train* y *test* mantienen estad√≠sticos similares de `y` (media/desv/rango) para evitar un *split* accidentalmente sesgado.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0a612b1-8c83-4b7e-aa82-f8b98dbd3f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1070, 8) | Test: (268, 8)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 2) Split temprano (80/20)\n",
    "# =========================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape} | Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2da6f9e-7239-44d1-b9fb-19a9701ffc70",
   "metadata": {},
   "source": [
    "# üßº Preprocesamiento en Pipeline ‚Äî *Code3*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **ColumnTransformer (preprocessor):** aplica `StandardScaler()` **solo** a las columnas num√©ricas (que aqu√≠ incluyen las dummies), con `remainder=\"drop\"` para usar √∫nicamente esas columnas.\n",
    "- **Por qu√© todas son num√©ricas:** ya hiciste `get_dummies`, as√≠ que `sex`, `smoker`, `region` quedaron como 0/1.  \n",
    "- **VarianceThreshold(0.0):** elimina columnas **constantes** (varianza cero). √ötil como red de seguridad ante features degeneradas; en este dataset no deber√≠a eliminar ninguna.\n",
    "- **ImbPipeline:** se usa el `Pipeline` de `imblearn` por consistencia de API; **no** se aplica SMOTE (es regresi√≥n).  \n",
    "- **build_pipe(model):** crea un pipeline ordenado ‚Üí `(\"prep\" ‚Üí \"var0\" ‚Üí \"model\")` para evitar **leakage**: el escalado se ajusta **solo con train** y luego se aplica a test.\n",
    "\n",
    "### ‚ú® Efecto del escalado\n",
    "- **Beneficia**: modelos sensibles a escala (Regresi√≥n Lineal/Ridge/Lasso, SVR, MLP).  \n",
    "- **Neutro**: √°rboles/RandomForest/GBDT (no necesitan escalado, pero no molesta).  \n",
    "- **Dummies 0/1**: escalarlas no da√±a; quedan centradas y con varianza unitaria junto al resto.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado del Code3 (tus salidas)\n",
    "- **Features num√©ricas (8):**  \n",
    "  `['age', 'bmi', 'children', 'sex_male', 'smoker_yes', 'region_northwest', 'region_southeast', 'region_southwest']`\n",
    "- **Features categ√≥ricas:** `[]`  \n",
    "‚úîÔ∏è Consistente con el *one-hot* previo y con el plan de escalar **todas** las columnas usadas por el modelo.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7c6dca-37f3-469d-8842-e333cbda85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features num√©ricas: ['age', 'bmi', 'children', 'sex_male', 'smoker_yes', 'region_northwest', 'region_southeast', 'region_southwest']\n",
      "Features categ√≥ricas: []\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3) Preprocesamiento (en pipeline)\n",
    "# =========================================\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline  # imblearn solo por consistencia de API\n",
    "\n",
    "# Como ya hicimos get_dummies, todas son num√©ricas\n",
    "cat_features = []\n",
    "num_features = X_train.columns.tolist()\n",
    "\n",
    "# Preprocesador: solo escalado\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_features),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "def build_pipe(model):\n",
    "    # Nota: en regresi√≥n NO se usa SMOTE\n",
    "    return ImbPipeline([\n",
    "        (\"prep\", preprocessor),\n",
    "        (\"var0\", VarianceThreshold(0.0)),  # limpia columnas constantes\n",
    "        (\"model\", model),\n",
    "    ])\n",
    "\n",
    "print(f\"Features num√©ricas: {num_features}\")\n",
    "print(f\"Features categ√≥ricas: {cat_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8729c-200e-44be-849f-a5f44b4ae8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost lightgbm catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd7b41-298c-40d3-86aa-7f13532a7310",
   "metadata": {},
   "source": [
    "# ü§ñ Modelos candidatos (Regresi√≥n) ‚Äî *Code4*\n",
    "\n",
    "## üß© ¬øQu√© define este bloque?\n",
    "Se arma una **lista de candidatos** `candidates` con modelos de regresi√≥n que cubren distintas hip√≥tesis (linealidad, no linealidad, interacciones) y diferentes sesgos/varianzas.  \n",
    "Cada modelo se entrenar√° dentro del **pipeline** (`build_pipe`) que ya escala y filtra varianza cero.\n",
    "\n",
    "**Lineales (baseline y regularizados)**\n",
    "- **LR ‚Äî LinearRegression()**: l√≠nea base sin regularizaci√≥n; sensible a colinealidad y outliers.\n",
    "- **RG ‚Äî Ridge(random_state=42)**: L2; reduce varianza, robusto ante colinealidad.\n",
    "- **LS ‚Äî Lasso(max_iter=5000, random_state=42)**: L1; hace selecci√≥n de variables (coeficientes a 0).\n",
    "- **EN ‚Äî ElasticNet(max_iter=5000, random_state=42)**: mezcla L1+L2; balancea selecci√≥n y estabilidad.\n",
    "\n",
    "**Basado en instancias**\n",
    "- **KNR ‚Äî KNeighborsRegressor()**: no param√©trico; depende de la **escala** (por eso escalamos). Sensible a ruido y dimensi√≥n.\n",
    "\n",
    "**√Årboles y *ensembles***\n",
    "- **DTR ‚Äî DecisionTreeRegressor(random_state=42)**: no lineal, captura interacciones; alto riesgo de **overfitting** sin poda.\n",
    "- **RFR ‚Äî RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1)**: promedia muchos √°rboles ‚Üí menor varianza; robusto a outliers; poco interpretable globalmente.\n",
    "\n",
    "**Red neuronal**\n",
    "- **MLP ‚Äî MLPRegressor(hidden_layer_sizes=(64,), max_iter=800, random_state=42)**: puede modelar no linealidades; requiere buen escalado y tuning (capas, *learning rate*, regularizaci√≥n).\n",
    "\n",
    "**Gradient Boosting (√°rboles potenciados)**\n",
    "- **XGB ‚Äî XGBRegressor(..., n_estimators=400, learning_rate=0.05, max_depth=6, subsample=0.9, colsample_bytree=0.9)**  \n",
    "- **LGB ‚Äî LGBMRegressor(..., n_estimators=500, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9)**  \n",
    "- **CAT ‚Äî CatBoostRegressor(iterations=600, depth=6, learning_rate=0.05, verbose=False)**  \n",
    "Todos capturan **no linealidad** e **interacciones** de forma eficiente; suelen rendir muy bien en tabulares. Aqu√≠ las categ√≥ricas ya est√°n en *one-hot*, por lo que CatBoost operar√° solo con num√©ricas (tambi√©n funciona)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fbc8485-a82e-4c51-8a61-c2a4955fb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 4) Modelos candidatos (REGRESI√ìN)\n",
    "# =========================================\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "candidates = [\n",
    "    (\"LR\",  LinearRegression()),\n",
    "    (\"RG\",  Ridge(random_state=RANDOM_STATE)),\n",
    "    (\"LS\",  Lasso(random_state=RANDOM_STATE, max_iter=5000)),\n",
    "    (\"EN\",  ElasticNet(random_state=RANDOM_STATE, max_iter=5000)),\n",
    "    (\"KNR\", KNeighborsRegressor()),\n",
    "    (\"DTR\", DecisionTreeRegressor(random_state=RANDOM_STATE)),\n",
    "    (\"RFR\", RandomForestRegressor(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1)),\n",
    "    (\"MLP\", MLPRegressor(hidden_layer_sizes=(64,), max_iter=800, random_state=RANDOM_STATE)),\n",
    "    (\"XGB\", XGBRegressor(tree_method=\"hist\", random_state=RANDOM_STATE,\n",
    "                         n_estimators=400, learning_rate=0.05, max_depth=6,\n",
    "                         subsample=0.9, colsample_bytree=0.9, n_jobs=-1)),\n",
    "    (\"LGB\", LGBMRegressor(n_estimators=500, learning_rate=0.05, max_depth=-1,\n",
    "                          subsample=0.9, colsample_bytree=0.9,\n",
    "                          random_state=RANDOM_STATE, n_jobs=-1, verbosity=-1)),\n",
    "    (\"CAT\", CatBoostRegressor(iterations=600, learning_rate=0.05, depth=6,\n",
    "                              random_state=RANDOM_STATE, l2_leaf_reg=3.0,\n",
    "                              verbose=False, allow_writing_files=False, thread_count=-1)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f80b0b-5bbc-486d-80dc-8d6b38459b14",
   "metadata": {},
   "source": [
    "## üìä Resultados e interpretaci√≥n\n",
    "\n",
    "### Ranking (promedios en test CV)\n",
    "1. **CAT** ‚Äî RMSE **4809.92**, MAE **2745.85**, R¬≤ **0.838** ‚úÖ  \n",
    "2. **RFR** ‚Äî RMSE 4940.32, MAE 2810.94, R¬≤ 0.830  \n",
    "3. **XGB** ‚Äî RMSE 5156.30, MAE 3071.67, R¬≤ 0.815  \n",
    "4. **LGB** ‚Äî RMSE 5171.88, MAE 3205.26, R¬≤ 0.814  \n",
    "5. **KNR** ‚Äî RMSE 5629.18, MAE 3507.49, R¬≤ 0.779  \n",
    "6‚Äì8. **LS / RG / LR** ‚Äî RMSE ~**6123**, MAE ~**4235**, R¬≤ ~**0.739** (l√≠nea base lineal/regularizada)  \n",
    "9. **DTR** ‚Äî RMSE 6671.85, MAE 3237.99, R¬≤ 0.689  \n",
    "10. **EN** ‚Äî RMSE 7050.55, MAE 5164.37, R¬≤ 0.654  \n",
    "11. **MLP** ‚Äî RMSE 15606.54, MAE 11280.46, R¬≤ **-0.693** (‚ö†Ô∏è no convergi√≥; ver aviso)\n",
    "\n",
    "### Lecturas clave\n",
    "- **Ganador baseline:** **CatBoostRegressor** (mejor RMSE y mejor R¬≤). Los **ensembles de √°rboles** dominan el problema (CAT ‚âà RF ‚âà XGB/LGB), lo esperado en datos tabulares con no linealidades e interacciones (p. ej., `smoker_yes √ó bmi √ó age`).\n",
    "- **Lineales (LR/Ridge/Lasso):** rendimiento s√≥lido pero **inferior** a √°rboles potenciados; capturan solo relaciones principalmente aditivas.\n",
    "- **KNN:** razonable pero por detr√°s de ensembles; sensible a escala y a la elecci√≥n de `n_neighbors` (a√∫n sin tuning).\n",
    "- **√Årbol simple (DTR):** peor que sus versiones en conjunto (**RF/Boosting**) por **alta varianza**.\n",
    "- **ElasticNet:** debajo de Ridge/Lasso sin tuning; la mezcla L1+L2 no ayud√≥ con hiperpar√°metros por defecto.\n",
    "- **MLP:** **ConvergenceWarning** y desempe√±o muy pobre ‚Üí requiere **early stopping**, regularizaci√≥n, ajuste de arquitectura y `max_iter` mayor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b245143-7bd0-4971-999f-4955066c979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LR | RMSE 6123.354 | MAE 4234.984 | R¬≤ 0.739\n",
      " RG | RMSE 6123.310 | MAE 4236.462 | R¬≤ 0.739\n",
      " LS | RMSE 6123.308 | MAE 4234.844 | R¬≤ 0.739\n",
      " EN | RMSE 7050.553 | MAE 5164.369 | R¬≤ 0.654\n",
      "KNR | RMSE 5629.177 | MAE 3507.490 | R¬≤ 0.779\n",
      "DTR | RMSE 6671.849 | MAE 3237.991 | R¬≤ 0.689\n",
      "RFR | RMSE 4940.323 | MAE 2810.938 | R¬≤ 0.830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (800) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP | RMSE 15606.537 | MAE 11280.464 | R¬≤ -0.693\n",
      "XGB | RMSE 5156.305 | MAE 3071.671 | R¬≤ 0.815\n",
      "LGB | RMSE 5171.876 | MAE 3205.260 | R¬≤ 0.814\n",
      "CAT | RMSE 4809.922 | MAE 2745.851 | R¬≤ 0.838\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAT</td>\n",
       "      <td>4809.921630</td>\n",
       "      <td>2745.851251</td>\n",
       "      <td>0.838345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RFR</td>\n",
       "      <td>4940.323260</td>\n",
       "      <td>2810.937874</td>\n",
       "      <td>0.829974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGB</td>\n",
       "      <td>5156.304618</td>\n",
       "      <td>3071.671276</td>\n",
       "      <td>0.814907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGB</td>\n",
       "      <td>5171.875600</td>\n",
       "      <td>3205.260495</td>\n",
       "      <td>0.813986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNR</td>\n",
       "      <td>5629.177182</td>\n",
       "      <td>3507.489693</td>\n",
       "      <td>0.779218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LS</td>\n",
       "      <td>6123.307570</td>\n",
       "      <td>4234.844428</td>\n",
       "      <td>0.738858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RG</td>\n",
       "      <td>6123.310369</td>\n",
       "      <td>4236.461734</td>\n",
       "      <td>0.738857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>6123.353823</td>\n",
       "      <td>4234.983570</td>\n",
       "      <td>0.738853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DTR</td>\n",
       "      <td>6671.848774</td>\n",
       "      <td>3237.991359</td>\n",
       "      <td>0.688597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EN</td>\n",
       "      <td>7050.553194</td>\n",
       "      <td>5164.369141</td>\n",
       "      <td>0.654384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MLP</td>\n",
       "      <td>15606.537204</td>\n",
       "      <td>11280.463849</td>\n",
       "      <td>-0.693271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model          rmse           mae        r2\n",
       "0    CAT   4809.921630   2745.851251  0.838345\n",
       "1    RFR   4940.323260   2810.937874  0.829974\n",
       "2    XGB   5156.304618   3071.671276  0.814907\n",
       "3    LGB   5171.875600   3205.260495  0.813986\n",
       "4    KNR   5629.177182   3507.489693  0.779218\n",
       "5     LS   6123.307570   4234.844428  0.738858\n",
       "6     RG   6123.310369   4236.461734  0.738857\n",
       "7     LR   6123.353823   4234.983570  0.738853\n",
       "8    DTR   6671.848774   3237.991359  0.688597\n",
       "9     EN   7050.553194   5164.369141  0.654384\n",
       "10   MLP  15606.537204  11280.463849 -0.693271"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Baseline ganador: CAT\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 5) Baseline con CV (sin tuning)\n",
    "# =========================================\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "import pandas as pd\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "scoring = {\n",
    "    \"rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"mae\":  \"neg_mean_absolute_error\",\n",
    "    \"r2\":   \"r2\",\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, model in candidates:\n",
    "    pipe = build_pipe(model)\n",
    "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
    "    row = {\n",
    "        \"model\": name,\n",
    "        \"rmse\": -scores[\"test_rmse\"].mean(),\n",
    "        \"mae\":  -scores[\"test_mae\"].mean(),\n",
    "        \"r2\":    scores[\"test_r2\"].mean(),\n",
    "    }\n",
    "    rows.append(row)\n",
    "    print(f\"{name:>3} | RMSE {row['rmse']:.3f} | MAE {row['mae']:.3f} | R¬≤ {row['r2']:.3f}\")\n",
    "\n",
    "baseline_df = pd.DataFrame(rows).sort_values(\"rmse\").reset_index(drop=True)\n",
    "display(baseline_df)\n",
    "\n",
    "baseline_best_name  = baseline_df.iloc[0][\"model\"]\n",
    "baseline_best_model = dict(candidates)[baseline_best_name]\n",
    "print(f\">>> Baseline ganador: {baseline_best_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92b55c-80b0-4fd5-8bb9-6316f68b0678",
   "metadata": {},
   "source": [
    "# üîß Tuning con CV y elecci√≥n del ganador ‚Äî *Code6*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Estrategia de b√∫squeda:** `RandomizedSearchCV` sobre un **pipeline** (`prep ‚Üí var0 ‚Üí model`) para evitar *leakage*.\n",
    "- **Esquemas de CV:**\n",
    "  - **Modelos ‚Äúligeros‚Äù** (RG, EN): `KFold(n_splits=5, shuffle=True, random_state=42)`.\n",
    "  - **Modelos ‚Äúpesados‚Äù** (RFR, XGB, LGB, CAT): `KFold(n_splits=3, ...)` para acelerar.\n",
    "- **Espacios de hiperpar√°metros:** distribuciones amplias (p. ej., `loguniform` para `alpha`/`learning_rate`, `randint` para profundidad/√°rboles/hojas).\n",
    "- **Par√°metros clave del `RandomizedSearchCV`:**\n",
    "  - `n_iter`: 12 para ligeros, **15** para pesados.\n",
    "  - `scoring`: **RMSE/MAE/R¬≤**; **`refit=\"rmse\"`** ‚Üí el mejor se elige por **menor RMSE** en CV.\n",
    "  - `n_jobs=-1`, `random_state=42`, `error_score=np.nan`.\n",
    "- **Cache opcional:** `pipe.set_params(memory=cache_dir)` (si el estimador lo soporta) para reutilizar transformaciones y **acelerar**.\n",
    "- **Salida final:** colecciona `(nombre, mejor_estimator, mejor_RMSE_CV, mejores_params)` y **ordena** por RMSE para declarar el **ganador optimizado**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Resultado e interpretaci√≥n (tus salidas)\n",
    "**Logs de ajuste:**\n",
    "- RG (12√ó5 folds) ‚Üí 60 *fits*  \n",
    "- EN (12√ó5 folds) ‚Üí 60 *fits*  \n",
    "- RFR/XGB/LGB/CAT (15√ó3 folds) ‚Üí 45 *fits* cada uno\n",
    "\n",
    "**Ganador optimizado (seg√∫n RMSE CV):**  \n",
    "> **RFR ‚Äî RandomForestRegressor**  \n",
    "> **RMSE CV = 4559.799** (menor es mejor)  \n",
    "> **Mejores hiperpar√°metros:**\n",
    "> - `bootstrap`: **True**  \n",
    "> - `max_depth`: **5**  \n",
    "> - `max_features`: **None**  \n",
    "> - `min_samples_leaf`: **7**  \n",
    "> - `min_samples_split`: **13**  \n",
    "> - `n_estimators`: **463**\n",
    "\n",
    "### üß† Lecturas clave\n",
    "- **Mejora vs. baseline CV:** el mejor baseline fue **CAT** con RMSE ‚âà **4809.92** (Code5, 5-fold). El tuning produjo **RFR 4559.80** (3-fold). **Indicativamente mejora** (~250 pts RMSE), aunque:\n",
    "  - ‚ö†Ô∏è **Advertencia de comparabilidad:** los **folds** no son id√©nticos (CAT baseline en 5-fold vs RFR tuning en 3-fold). Aun as√≠, la magnitud de mejora sugiere **progreso real**. Se debe **validar en TEST**.\n",
    "- **Hiperpar√°metros aprendidos**:\n",
    "  - `max_depth=5`, `min_samples_leaf=7`, `min_samples_split=13` ‚Üí **control de complejidad** para evitar sobreajuste.\n",
    "  - `n_estimators=463` garantiza estabilidad de la media del bosque.\n",
    "  - `max_features=None` (toma todas las features por divisi√≥n) puede mejorar ajuste en datasets con pocas columnas (8), compensado por la poda suave (`depth`/`leaf`/`split`).\n",
    "- **Siguiente paso cr√≠tico:** evaluar el **pipeline √≥ptimo** en el **conjunto de test** (Code2) y comparar **RMSE/MAE/R¬≤** con el baseline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f232e5-0ca2-4e97-9cc8-9b074a6f8f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      ">>> GANADOR OPTIMIZADO: RFR (RMSE CV=4559.799)\n",
      "Mejores hiperpar√°metros encontrados: {'model__bootstrap': True, 'model__max_depth': 5, 'model__max_features': None, 'model__min_samples_leaf': 7, 'model__min_samples_split': 13, 'model__n_estimators': 463}\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 6) Tuning con CV y elecci√≥n del ganador (r√°pido) para tu dataset\n",
    "# =========================================\n",
    "import tempfile, shutil\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "try:\n",
    "    from scipy.stats import loguniform\n",
    "except Exception:\n",
    "    from sklearn.utils.fixes import loguniform\n",
    "\n",
    "# CV: m√°s fuerte para modelos ligeros, m√°s suave para pesados\n",
    "cv_light = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_heavy = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Espacios de hiperpar√°metros\n",
    "param_spaces = {\n",
    "    \"RG\":  {\"model__alpha\": loguniform(1e-3, 1e3)},\n",
    "    \"EN\":  {\"model__alpha\": loguniform(1e-3, 1e2), \"model__l1_ratio\": uniform(0.0, 1.0)},\n",
    "    \"RFR\": {\"model__n_estimators\": randint(200, 600), \"model__max_depth\": randint(4, 16),\n",
    "            \"model__min_samples_split\": randint(2, 20), \"model__min_samples_leaf\": randint(1, 10),\n",
    "            \"model__max_features\": [\"sqrt\",\"log2\", None], \"model__bootstrap\": [True, False]},\n",
    "    \"XGB\": {\"model__n_estimators\": randint(250, 600), \"model__learning_rate\": loguniform(5e-3, 2e-1),\n",
    "            \"model__max_depth\": randint(3, 9), \"model__subsample\": uniform(0.7, 0.3),\n",
    "            \"model__colsample_bytree\": uniform(0.7, 0.3), \"model__min_child_weight\": randint(1, 6)},\n",
    "    \"LGB\": {\"model__n_estimators\": randint(300, 800), \"model__learning_rate\": loguniform(5e-3, 2e-1),\n",
    "            \"model__num_leaves\": randint(16, 128), \"model__max_depth\": randint(-1, 12),\n",
    "            \"model__min_child_samples\": randint(10, 50), \"model__subsample\": uniform(0.7, 0.3),\n",
    "            \"model__colsample_bytree\": uniform(0.7, 0.3), \"model__reg_lambda\": loguniform(1e-3, 10)},\n",
    "    \"CAT\": {\"model__iterations\": randint(300, 700), \"model__learning_rate\": loguniform(5e-3, 2e-1),\n",
    "            \"model__depth\": randint(4, 10), \"model__l2_leaf_reg\": loguniform(1e-2, 30),\n",
    "            \"model__border_count\": randint(32, 255)},\n",
    "}\n",
    "\n",
    "# Modelos a optimizar\n",
    "to_tune = [\n",
    "    (\"RG\",  Ridge(random_state=RANDOM_STATE)),\n",
    "    (\"EN\",  ElasticNet(random_state=RANDOM_STATE, max_iter=5000)),\n",
    "    (\"RFR\", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=1)),\n",
    "    (\"XGB\", XGBRegressor(tree_method=\"hist\", random_state=RANDOM_STATE, n_jobs=1)),\n",
    "    (\"LGB\", LGBMRegressor(random_state=RANDOM_STATE, n_jobs=1, verbosity=-1)),\n",
    "    (\"CAT\", CatBoostRegressor(random_state=RANDOM_STATE, verbose=False, allow_writing_files=False, thread_count=1)),\n",
    "]\n",
    "\n",
    "# M√©tricas de evaluaci√≥n\n",
    "refit_metric = \"rmse\"  # minimizar RMSE\n",
    "scoring = {\"rmse\": \"neg_root_mean_squared_error\", \n",
    "           \"mae\": \"neg_mean_absolute_error\", \n",
    "           \"r2\": \"r2\"}\n",
    "\n",
    "# Lista de resultados\n",
    "best_models = []\n",
    "cache_dir = tempfile.mkdtemp(prefix=\"skcache_\")\n",
    "\n",
    "try:\n",
    "    for name, base_model in to_tune:\n",
    "        pipe = build_pipe(base_model)\n",
    "        try:\n",
    "            pipe.set_params(memory=cache_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        heavy = name in [\"RFR\",\"XGB\",\"LGB\",\"CAT\"]\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            pipe,\n",
    "            param_spaces[name],\n",
    "            n_iter=(15 if heavy else 12),\n",
    "            cv=(cv_heavy if heavy else cv_light),\n",
    "            scoring=scoring,\n",
    "            refit=\"rmse\",\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=1,\n",
    "            error_score=np.nan,\n",
    "            return_train_score=False\n",
    "        )\n",
    "\n",
    "        search.fit(X_train, y_train)\n",
    "        best_models.append((name, search.best_estimator_, -search.best_score_, search.best_params_))  # RMSE positivo\n",
    "\n",
    "    # Ordenar por menor RMSE\n",
    "    best_models.sort(key=lambda x: x[2])\n",
    "    best_name, final_pipe_opt, best_cv_rmse, best_params = best_models[0]\n",
    "\n",
    "    print(f\">>> GANADOR OPTIMIZADO: {best_name} (RMSE CV={best_cv_rmse:.3f})\")\n",
    "    print(\"Mejores hiperpar√°metros encontrados:\", best_params)\n",
    "\n",
    "finally:\n",
    "    shutil.rmtree(cache_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9822ec4-714b-49a0-9196-bd15f78c311b",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Comparaci√≥n justa (solo CV) ‚Äî *Code7*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Mismo esquema de CV para ambos**: `same_cv = KFold(n_splits=5, shuffle=True, random_state=123)`  \n",
    "  ‚Üí garantiza una **comparaci√≥n manzana con manzana** (mismos folds para baseline y tuned).\n",
    "- **Pipelines comparados**:\n",
    "  - `pipe_baseline_best`: pipeline con el **mejor baseline** de Code5 (`baseline_best_model`).\n",
    "  - `pipe_tuned_best`  : pipeline **optimizado** de Code6 (`final_pipe_opt`).\n",
    "- **M√©trica usada**: **RMSE** (menor es mejor) computada con `cross_validate` en los **mismos folds**.\n",
    "- **Regla de decisi√≥n**:\n",
    "  - Si la mejora relativa \\((\\text{RMSE}_{base} - \\text{RMSE}_{tuned}) / \\text{RMSE}_{base} \\ge 1\\%\\) ‚Üí **gana el tuned**.\n",
    "  - Si no, **nos quedamos con el baseline** por **simplicidad** y menor riesgo.\n",
    "- **Salida esperada**:\n",
    "  - Imprime dos l√≠neas tipo:  \n",
    "    `Baseline(CAT): RMSE 4xxx.xxxx`  \n",
    "    `Tuned(RFR): RMSE 4xxx.xxxx`  \n",
    "  - Luego: `>>> Modelo seleccionado para TEST: <nombre>`\n",
    "\n",
    "---\n",
    "\n",
    "## üîç C√≥mo interpretar la salida\n",
    "- **Caso A ‚Äî Tuned mejora ‚â• 1%**  \n",
    "  - *Ej.* `Baseline(CAT): RMSE 4810` vs `Tuned(RFR): RMSE 4560`  \n",
    "  - **Conclusi√≥n**: el **tuned** ofrece **ganancia real** y consistente en los mismos folds ‚Üí **avanzar con tuned** a evaluaci√≥n en **TEST**.\n",
    "- **Caso B ‚Äî Mejora < 1%**  \n",
    "  - *Ej.* `Baseline(CAT): RMSE 4810` vs `Tuned(RFR): RMSE 4780`  \n",
    "  - **Conclusi√≥n**: la diferencia es marginal; por **parquedad** y **robustez**, quedarse con el **baseline**.\n",
    "- **Buenas pr√°cticas**:\n",
    "  - Reporta el **% de mejora**: \\(\\Delta\\% = 100 \\times (\\text{RMSE}_{base} - \\text{RMSE}_{tuned}) / \\text{RMSE}_{base}\\).\n",
    "  - Si dudas por **varianza**, repite con otro `random_state` o usa **repeated KFold** para estimar la **incertidumbre** de la mejora.\n",
    "  - El ganador de esta secci√≥n es el que se usar√° en el **TEST hold-out** (Code2) en el siguiente paso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01b5e18a-312c-493f-b3c1-31a0d8ea58ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline(CAT): RMSE 4832.2032\n",
      "Tuned(RFR): RMSE 4571.8756\n",
      ">>> Modelo seleccionado para TEST: RFR\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 7) Comparaci√≥n justa (solo CV) - baseline vs ganador\n",
    "# =========================================\n",
    "from sklearn.model_selection import KFold, cross_validate\n",
    "\n",
    "same_cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "pipe_baseline_best = build_pipe(baseline_best_model)\n",
    "pipe_tuned_best    = final_pipe_opt\n",
    "\n",
    "def cv_rmse(pipe, name):\n",
    "    s = cross_validate(pipe, X_train, y_train, cv=same_cv,\n",
    "                       scoring={\"rmse\":\"neg_root_mean_squared_error\"}, n_jobs=-1)\n",
    "    rmse = -s[\"test_rmse\"].mean()\n",
    "    print(f\"{name}: RMSE {rmse:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "rmse_base = cv_rmse(pipe_baseline_best, f\"Baseline({baseline_best_name})\")\n",
    "rmse_tune = cv_rmse(pipe_tuned_best,   f\"Tuned({best_name})\")\n",
    "\n",
    "# Regla: si la mejora < 1% del RMSE base, nos quedamos con el baseline (m√°s simple)\n",
    "if (rmse_base - rmse_tune) / rmse_base >= 0.01:\n",
    "    winner_name, winner_pipe = best_name, pipe_tuned_best\n",
    "else:\n",
    "    winner_name, winner_pipe = baseline_best_name, pipe_baseline_best\n",
    "\n",
    "print(f\">>> Modelo seleccionado para TEST: {winner_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b85f2b-83e9-453e-aa73-38d4bc4a65c5",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Pol√≠tica de decisi√≥n (postprocesado) ‚Äî *Code8*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Define una pol√≠tica** (`POLICY`) para ajustar las predicciones antes de evaluarlas/desplegarlas:\n",
    "  - `clip_to_train_range=True` ‚Üí **recorta** predicciones al **rango observado en TRAIN**.\n",
    "  - `round_to_int=False` ‚Üí no redondea (apropiado para `charges`, variable continua).\n",
    "  - `lower` / `upper` ‚Üí l√≠mites tomados de `y_train.min()` y `y_train.max()`.\n",
    "- **Funci√≥n `postprocess_preds`**:\n",
    "  - Copia `yhat`.\n",
    "  - Aplica `np.clip(yhat, lower, upper)` si el recorte est√° activo.\n",
    "  - Redondea a entero solo si `round_to_int=True`.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç C√≥mo interpretar la salida\n",
    "- **Prop√≥sito del recorte:** evitar **extrapolaciones extremas** (negativas o fuera de escala) y estabilizar el comportamiento del modelo en colas.\n",
    "- **L√≠mites basados en TRAIN:** previene *leakage* (no usa informaci√≥n de TEST).  \n",
    "- **Riesgo:** si en producci√≥n hay valores reales fuera del rango de TRAIN, el recorte puede introducir **sesgo** (sub/sobreestimaci√≥n en extremos).\n",
    "- **Redondeo:** mantener **`False`** para `charges`; activar solo si el objetivo es **discreto** (conteos).\n",
    "\n",
    "**Chequeo recomendado (opcional):** calcular **% de predicciones recortadas** y comparar m√©tricas **antes vs. despu√©s** del postprocesado para verificar su impacto.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9156f243-260e-43cc-b487-d5a1eb1f0fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pol√≠tica: {'clip_to_train_range': True, 'round_to_int': False, 'lower': 1121.8739, 'upper': 62592.87309}\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 8) Pol√≠tica de decisi√≥n (m√≠nima)\n",
    "# =========================================\n",
    "POLICY = {\n",
    "    \"clip_to_train_range\": True,   # recorta predicciones al rango visto en TRAIN\n",
    "    \"round_to_int\": False,         # pon True si el objetivo es entero (conteos)\n",
    "    \"lower\": float(y_train.min()),\n",
    "    \"upper\": float(y_train.max()),\n",
    "}\n",
    "print(\"Pol√≠tica:\", POLICY)\n",
    "\n",
    "def postprocess_preds(yhat, policy=POLICY):\n",
    "    ypp = yhat.copy()\n",
    "    if policy.get(\"clip_to_train_range\", False):\n",
    "        ypp = np.clip(ypp, policy[\"lower\"], policy[\"upper\"])\n",
    "    if policy.get(\"round_to_int\", False):\n",
    "        ypp = np.rint(ypp).astype(int)\n",
    "    return ypp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed39b054-eebd-4e24-a9b6-ab806b0d6f33",
   "metadata": {},
   "source": [
    "# üß™ Evaluaci√≥n final en TEST ‚Äî *Code9*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Entrena** el `winner_pipe` con `X_train, y_train`.\n",
    "- **Predice** en `X_test` y aplica el **postprocesado** (`postprocess_preds`) seg√∫n la **POLICY** (recorte al rango de TRAIN).\n",
    "- **Calcula m√©tricas** en TEST:\n",
    "  - **RMSE** (ra√≠z del error cuadr√°tico medio) ‚Äî penaliza m√°s los errores grandes.\n",
    "  - **MAE** (error absoluto medio) ‚Äî interpretable como error promedio en unidades de `charges`.\n",
    "  - **R¬≤** ‚Äî proporci√≥n de varianza explicada.\n",
    "- **Muestra** un **vistazo de 10 casos** (`y_true` vs `y_pred`) para inspecci√≥n r√°pida.\n",
    "\n",
    "---\n",
    "## üîç Interpretaci√≥n\n",
    "- **Desempe√±o global**:\n",
    "  - **R¬≤ = 0.8787** ‚Üí el modelo explica ~**87.9%** de la varianza de `charges` en **datos no vistos**.\n",
    "  - **RMSE ‚âà 4,339** y **MAE ‚âà 2,471**. Con media de `charges` ‚âà **13,270** (Code1), esto implica:\n",
    "    - `RMSE / media ‚âà 33%` ‚Üí error t√≠pico cuadr√°tico en torno a un tercio de la media.\n",
    "    - `MAE / media ‚âà 19%` ‚Üí error absoluto promedio cercano a una quinta parte de la media.\n",
    "- **Consistencia vs CV**:\n",
    "  - El tuning report√≥ **RMSE CV ‚âà 4,560** (Code6). En TEST obtienes **4,339**, **ligeramente mejor** (posible variaci√≥n de muestra; se√±al de **generalizaci√≥n razonable**).\n",
    "- **Patr√≥n en ejemplos**:\n",
    "  - Hay **sobreestimaciones** moderadas (p. ej., 9,095 ‚Üí 10,253; +1,157) y **subestimaciones** en casos altos (p. ej., 29,331 ‚Üê 27,553; ‚àí1,778), coherentes con el compromiso sesgo-varianza del bosque.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "292e9f62-5cb2-4de1-85c6-6ebcf4c98acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST ‚Üí RMSE: 4339.3197 | MAE: 2470.5496 | R¬≤: 0.8787\n",
      "     y_true       y_pred\n",
      " 9095.06825 10252.584961\n",
      " 5272.17580  5932.748470\n",
      "29330.98315 27553.068281\n",
      " 9301.89355 10602.135929\n",
      "33750.29180 34827.732703\n",
      " 4536.25900  6569.321544\n",
      " 2117.33885  2345.542471\n",
      "14210.53595 14145.541554\n",
      " 3732.62510  5355.007769\n",
      "10264.44210 11524.080489\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 9) Evaluaci√≥n final en TEST\n",
    "# =========================================\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "winner_pipe.fit(X_train, y_train)\n",
    "y_pred = winner_pipe.predict(X_test)\n",
    "y_pp   = postprocess_preds(y_pred, POLICY)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pp, squared=False)\n",
    "mae  = mean_absolute_error(y_test, y_pp)\n",
    "r2   = r2_score(y_test, y_pp)\n",
    "\n",
    "print(f\"TEST ‚Üí RMSE: {rmse:.4f} | MAE: {mae:.4f} | R¬≤: {r2:.4f}\")\n",
    "\n",
    "# vistazo r√°pido (primeros 10)\n",
    "import pandas as pd\n",
    "preview = pd.DataFrame({\"y_true\": y_test.reset_index(drop=True),\n",
    "                        \"y_pred\": pd.Series(y_pp)}).head(10)\n",
    "print(preview.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8a01e-60ab-44cc-88e5-72062f8d226c",
   "metadata": {},
   "source": [
    "# üîé Interpretabilidad + An√°lisis de Error (m√≠nimo) ‚Äî *Code10*\n",
    "\n",
    "## üß© ¬øQu√© hace el c√≥digo?\n",
    "- **Recorte aplicado por la pol√≠tica:** calcula el % de predicciones que quedar√≠an fuera del rango de TRAIN y ser√≠an **recortadas**.\n",
    "- **Importancia por permutaci√≥n (pipeline completo):** mide cu√°nto **empeora el RMSE** cuando se desordena cada *feature* original, evaluando la importancia **en el flujo real** (prep + modelo).\n",
    "- **An√°lisis de errores:** construye `|error|` por fila, resume su **distribuci√≥n** y lista los **10 peores casos** con sus *features*.\n",
    "- **Subgrupos (opcional):** si existe `clase_salario`, reporta **MAE por grupo**.\n",
    "\n",
    "---\n",
    "## üîç Interpretaci√≥n\n",
    "- **Factor cr√≠tico:** *smoker_yes* es **clav√≠simo** (con diferencia) en la predicci√≥n de `charges`.  \n",
    "- **No linealidades/colas:** los grandes errores se concentran en **altos costos m√©dicos** (cola derecha). Esto es t√≠pico; incluso los ensembles pueden **subestimar** extremos.\n",
    "- **Variables con poco aporte:** regiones y sexo apenas mueven la aguja en este conjunto (posible se√±al de que su efecto est√° absorbido por las otras variables).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99525c07-ca07-4dc0-9ef3-02a1246ecb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Policy] clipped_low: 0.000% | clipped_high: 0.000%\n",
      "\n",
      "Top-15 importancias (perm, columnas originales):\n",
      "         feature   importance        std\n",
      "      smoker_yes 11001.294213 495.172353\n",
      "             bmi  3240.656661 254.882929\n",
      "             age  2547.839869 158.575499\n",
      "        children   207.444110  66.041193\n",
      "region_southwest     7.708388   5.929926\n",
      "region_southeast     4.997960   3.239566\n",
      "region_northwest     3.494475   4.086974\n",
      "        sex_male    -2.065253   5.029555\n",
      "\n",
      "Resumen de |error|:\n",
      "count      268.000000\n",
      "mean      2470.549641\n",
      "std       3574.037408\n",
      "min          6.906871\n",
      "10%        489.538277\n",
      "25%        925.408559\n",
      "50%       1517.673893\n",
      "75%       2305.828097\n",
      "90%       3599.116406\n",
      "max      21269.765057\n",
      "\n",
      "Peores 10 casos (|error| alto):\n",
      "     y_true       y_pred      abs_err  age    bmi  children  sex_male  smoker_yes  region_northwest  region_southeast  region_southwest\n",
      "28476.73499  7206.969933 21269.765057   40 41.420         1     False       False              True             False             False\n",
      "33471.97189 12415.611351 21056.360539   52 37.525         2     False       False              True             False             False\n",
      "23082.95533  2465.990159 20616.965171   19 33.100         0      True       False             False             False              True\n",
      "30259.99556 13588.805514 16671.190046   60 28.595         0      True       False             False             False             False\n",
      "63770.42801 47144.414354 16626.013656   54 47.410         0     False        True             False              True             False\n",
      "22493.65964  6310.185024 16183.474616   19 27.265         2      True       False              True             False             False\n",
      "32734.18630 16862.723353 15871.462947   32 17.765         2     False        True              True             False             False\n",
      "20177.67113  5592.561950 14585.109180   28 27.500         2     False       False             False             False              True\n",
      "20277.80751  5836.213423 14441.594087   29 29.640         1      True       False             False             False             False\n",
      "25333.33284 10969.650658 14363.682182   50 32.110         2      True       False             False             False             False\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 10) Interpretabilidad + breve error analysis (m√≠nimo, FIX)\n",
    "# =========================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 10.1 ¬øCu√°nto recorta la pol√≠tica?\n",
    "raw_pred = winner_pipe.predict(X_test)\n",
    "clip_low  = (raw_pred < POLICY[\"lower\"]).mean()\n",
    "clip_high = (raw_pred > POLICY[\"upper\"]).mean()\n",
    "print(f\"[Policy] clipped_low: {clip_low:.3%} | clipped_high: {clip_high:.3%}\")\n",
    "\n",
    "# 10.2 Importancias por Permutaci√≥n (sobre columnas ORIGINALES)\n",
    "r = permutation_importance(\n",
    "    winner_pipe,            # pipeline completa\n",
    "    X_test, y_test,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "\n",
    "feat_names = X_test.columns  # <-- CLAVE: mismos nombres que el X de entrada\n",
    "imp = (pd.DataFrame({\n",
    "        \"feature\": feat_names,\n",
    "        \"importance\": r.importances_mean,\n",
    "        \"std\": r.importances_std\n",
    "     })\n",
    "     .sort_values(\"importance\", ascending=False)\n",
    "     .head(15)\n",
    ")\n",
    "print(\"\\nTop-15 importancias (perm, columnas originales):\")\n",
    "print(imp.to_string(index=False))\n",
    "\n",
    "# 10.3 Errores: resumen + peores casos\n",
    "y_hat = winner_pipe.predict(X_test)\n",
    "y_pp  = postprocess_preds(y_hat, POLICY)\n",
    "res   = pd.DataFrame({\n",
    "    \"y_true\": y_test.reset_index(drop=True),\n",
    "    \"y_pred\": pd.Series(y_pp)\n",
    "})\n",
    "res[\"abs_err\"] = (res[\"y_true\"] - res[\"y_pred\"]).abs()\n",
    "print(\"\\nResumen de |error|:\")\n",
    "print(res[\"abs_err\"].describe(percentiles=[.1,.25,.5,.75,.9]).to_string())\n",
    "\n",
    "print(\"\\nPeores 10 casos (|error| alto):\")\n",
    "top_bad_idx = res[\"abs_err\"].nlargest(10).index\n",
    "print(pd.concat([res.loc[top_bad_idx], X_test.reset_index(drop=True).loc[top_bad_idx]], axis=1)\n",
    "      .to_string(index=False))\n",
    "\n",
    "# 10.4 M√©tricas por subgrupos (ej. clase_salario)\n",
    "if \"clase_salario\" in X_test.columns:\n",
    "    by_cls = (pd.concat([X_test.reset_index(drop=True)[[\"clase_salario\"]], res], axis=1)\n",
    "              .groupby(\"clase_salario\")[\"abs_err\"]\n",
    "              .agg([\"count\",\"mean\",\"median\"]))\n",
    "    print(\"\\nMAE por clase_salario:\")\n",
    "    print(by_cls.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd1a25c6-9487-4f6b-9487-2e519d11675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = winner_pipe.named_steps[\"prep\"].transform(X_test)\n",
    "model = winner_pipe.named_steps[\"model\"]\n",
    "r2 = permutation_importance(model, Xtr, y_test, n_repeats=10,\n",
    "                            random_state=RANDOM_STATE,\n",
    "                            scoring=\"neg_root_mean_squared_error\")\n",
    "feat_names_ohe = winner_pipe.named_steps[\"prep\"].get_feature_names_out()\n",
    "imp_ohe = pd.DataFrame({\"feature\": feat_names_ohe,\n",
    "                        \"importance\": r2.importances_mean,\n",
    "                        \"std\": r2.importances_std}).sort_values(\"importance\", ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17556694-e26a-4c8d-91fa-c7d84535776b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>num__smoker_yes</td>\n",
       "      <td>11001.294213</td>\n",
       "      <td>495.172353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>num__bmi</td>\n",
       "      <td>3240.656661</td>\n",
       "      <td>254.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>num__age</td>\n",
       "      <td>2547.839869</td>\n",
       "      <td>158.575499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>num__children</td>\n",
       "      <td>207.444110</td>\n",
       "      <td>66.041193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>num__region_southwest</td>\n",
       "      <td>7.708388</td>\n",
       "      <td>5.929926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num__region_southeast</td>\n",
       "      <td>4.997960</td>\n",
       "      <td>3.239566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num__region_northwest</td>\n",
       "      <td>3.494475</td>\n",
       "      <td>4.086974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num__sex_male</td>\n",
       "      <td>-2.065253</td>\n",
       "      <td>5.029555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 feature    importance         std\n",
       "4        num__smoker_yes  11001.294213  495.172353\n",
       "1               num__bmi   3240.656661  254.882929\n",
       "0               num__age   2547.839869  158.575499\n",
       "2          num__children    207.444110   66.041193\n",
       "7  num__region_southwest      7.708388    5.929926\n",
       "6  num__region_southeast      4.997960    3.239566\n",
       "5  num__region_northwest      3.494475    4.086974\n",
       "3          num__sex_male     -2.065253    5.029555"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c6d25-4f67-4aaf-a68b-b69b43d414d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
